#+title: My Common VM Setup
#+date: <2022-03-13 Sun 17:47>
#+hugo_lastmod: <2022-12-12 Mon 01:51>
#+hugo_tags: gaming kvm qemu pci_passthrough vm
#+setupfile: ./setup.conf

* Before Starting
This post mainly discusses VM setup for Windows since I've been using Windows as a secondary OS for apps or games that cannot run on Linux.

This post discusses setup on Arch Linux.

This post assumes the CPU and motherboard support =VT-d= and =IOMMU= features.  Detailed prerequisites can be found on [[https://wiki.archlinux.org/title/PCI_passthrough_via_OVMF][this page]].

* Install Hypervisor
Follow Arch wiki to install and setup:
1. [[https://wiki.archlinux.org/title/QEMU][QEMU]]
2. [[https://wiki.archlinux.org/title/libvirt][Libvirt]]
3. [[https://wiki.archlinux.org/title/Virt-Manager][Virt-Manager]]
4. [[https://wiki.archlinux.org/title/PCI_passthrough_via_OVMF][OVMF]]

* Install Windows VM
** Before Installation
Download the latest Windows 10 ISO from [[https://www.microsoft.com/en-ca/software-download/windows10ISO][Microsoft]].  Windows 11 is buggy and requires Microsoft account login during installation, which sucks.

We also need to get [[https://github.com/virtio-win/virtio-win-pkg-scripts/blob/master/README.md][VirtIO]] driver before installing Windows.

** During Installation
Then create a new VM with the following configuration before starting installation:
- Overview: Change /Firmware/ to =UEFI=.
- CPUs:
  - Unselect /Copy host CPU configuration/ and change /Model/ to =host-passthrough=.
  - Select /Manually set CPU topology/.  Change /Sockets/ to =1=, /Cores/ to =4=, /Threads/ to =2= (Physical core =4= * threads for each core =2=).
- Disk: Change /Disk bus/ to =VirtIO=.
- NIC: Change /Device model/ to =VirtIO=.
- Display Spice: Use /Type/ =Spice server=.
- Video: Use /Model/ =QXL=.

During the installation, the Windows installer may not be able to find the disk because it doesn't have the driver (that's why you have to download it beforehand).  Load the driver from the ISO and continue installation.  This step should be trivial.

** After Installation
Keep the =VirtIO= ISO in the CD ROM device when boot into Windows.  Find =virtio-win-gt-x64.msi= and =virtio-win-guest-tools.exe= then install them both.  Then power off VM.

Then configure VM:
- Add Hardware -> Channel -> Select =org.qemu.guest_agent.0= -> Finish

Until here, the basic Windows VM setup is done.

** Note
The video device doesn't support =VirtIO= [[https://wiki.archlinux.org/title/QEMU#virtio][on Windows]] so we have to use =QXL= for now.

* Passthrough: Discrete Graphic Card

** Identify Discrete Graphic Card
In a terminal:

#+begin_src shell
$ lspci -nnk | grep -A 3 -i nvidia
01:00.0 VGA compatible controller [0300]: NVIDIA Corporation GM204 [GeForce GTX 970] [10de:13c2] (rev a1)
    Subsystem: Gigabyte Technology Co., Ltd Device [1458:367a]
    Kernel driver in use: nouveau
    Kernel modules: nouveau
01:00.1 Audio device [0403]: NVIDIA Corporation GM204 High Definition Audio Controller [10de:0fbb] (rev a1)
    Subsystem: Gigabyte Technology Co., Ltd Device [1458:367a]
    Kernel driver in use: snd_hda_intel
    Kernel modules: snd_hda_intel
#+end_src

Take a note of the device IDs.  In this example I have a Nvidia GTX970 graphic card along with a audio controller.  They belong to the same group (domain) you have to take them all.

In this case I got =10de:13c2= and =10de:0fbb=.  These are the PCI devices that will be passed through to the VM.  Other PCI devices can be passed too.

** Add Kernel Modules
Add following modules to =mkinitcpio.conf=.

#+begin_src conf
# /etc/mkinitcpio.conf
MODULES=(vfio_pci vfio vfio_iommu_type1 vfio_virqfd)
#+end_src

Then regenerate ramfs.

#+begin_src shell
$ sudo mkinitcpio -P
#+end_src

** Add Kernel Parameters
Then we're going to enable IOMMU and prevent host Linux loading PCI devices that we want to passthrough to the VM.

The kernel parameter passing could be different depending on the bootloader you use.  In this example, I use =systemd-boot=.

Edit the system entry.  Add =intel_iommu=on= to the kernel parameter along with =vfio-pci.ids=10de:13c2,10de:0fbb= which contains the device IDs we got from the previous step.

#+begin_src conf
# /boot/loader/entries/arch.conf
options intel_iommu=on vfio-pci.ids=10de:13c2,10de:0fbb
#+end_src

After reboot, verify IOMMU is enabled successfully via =dmesg=.

#+begin_src shell
$ sudo dmesg | grep -i -e DMAR -e IOMMU
[    0.000000] ACPI: DMAR 0x00000000BDCB1CB0 0000B8 (v01 INTEL  BDW      00000001 INTL 00000001)
[    0.000000] Intel-IOMMU: enabled
[    0.028879] dmar: IOMMU 0: reg_base_addr fed90000 ver 1:0 cap c0000020660462 ecap f0101a
[    0.028883] dmar: IOMMU 1: reg_base_addr fed91000 ver 1:0 cap d2008c20660462 ecap f010da
[    0.028950] IOAPIC id 8 under DRHD base  0xfed91000 IOMMU 1
[    0.536212] DMAR: No ATSR found
[    0.536229] IOMMU 0 0xfed90000: using Queued invalidation
[    0.536230] IOMMU 1 0xfed91000: using Queued invalidation
[    0.536231] IOMMU: Setting RMRR:
[    0.536241] IOMMU: Setting identity map for device 0000:00:02.0 [0xbf000000 - 0xcf1fffff]
[    0.537490] IOMMU: Setting identity map for device 0000:00:14.0 [0xbdea8000 - 0xbdeb6fff]
[    0.537512] IOMMU: Setting identity map for device 0000:00:1a.0 [0xbdea8000 - 0xbdeb6fff]
[    0.537530] IOMMU: Setting identity map for device 0000:00:1d.0 [0xbdea8000 - 0xbdeb6fff]
[    0.537543] IOMMU: Prepare 0-16MiB unity mapping for LPC
[    0.537549] IOMMU: Setting identity map for device 0000:00:1f.0 [0x0 - 0xffffff]
[    2.182790] [drm] DMAR active, disabling use of stolen memory
#+end_src

Also, make sure VFIO driver is loaded.

#+begin_src shell
$ lspci -nnk | grep -i -A 3 nvidia
04:00.0 VGA compatible controller [0300]: NVIDIA Corporation GM204 [GeForce GTX 970] [10de:13c2] (rev a1)
	Subsystem: Gigabyte Technology Co., Ltd Device [1458:367a]
	Kernel driver in use: vfio-pci
	Kernel modules: nouveau
04:00.1 Audio device [0403]: NVIDIA Corporation GM204 High Definition Audio Controller [10de:0fbb] (rev a1)
	Subsystem: Gigabyte Technology Co., Ltd Device [1458:367a]
	Kernel driver in use: vfio-pci
	Kernel modules: snd_hda_intel
#+end_src

** Passthrough PCI
Open VM settings and add the discrete graphic card as well as it's audio controller etc.

** Install Drivers
Boot into Windows VM, now you should be able to install the graphic card driver.

*** Fix Error Code 43
The latest Windows 10/11 should not have this problem.  However, in any case if display adapter is malfunctioning and showing error code 43, use following workaround to avoid it.

Then open VM settings in XML view, add following content in the relevant sections to prevent Nvidia driver installer discovering the VM environment.  Some sections may exist already so just append to them.

#+begin_src xml
<domain>
  <features>
    <hyperv>
      <vendor_id state='on' value='1234567890ab'/>
    </hyperv>
    <kvm>
      <hidden state='on'>
    </kvm>
  </features>
</domain>
#+end_src

** Passthrough Enclosure Device
I'm using Razer Core Chroma and Synapse is not available on Linux.  In order to adjust RGB lights, pass the enclosure from USB device list.

** Notes
My setup is based on external GPU.  If eGPU is powered on when laptop starts, somehow the kernel will be stuck.  The eGPU has to be powered off and then powered on when system is up.  Weird.

* Passthrough: Integrated Graphic Card with SR-IOV
I have a fairly new laptop with 12th gen Intel and Iris Xe graphic card but no discrete one.  In this case Intel has provided a technology call =SR-IOV= which allows me to passthrough a part of my iGPU to the VM.

Note: =GVT-g= is [[https://wiki.archlinux.org/title/Intel_GVT-g#Prerequisite][only supported on 5th gen to 10th gen Intel CPUs]].

** Install Kernel Module
I was following this [[https://github.com/intel/linux-intel-lts/issues/33][GitHub issue]].  Basically an Intel customized =i915= kernel module is needed.  However, thanks to @strongtz who has created an [[https://aur.archlinux.org/packages/i915-sriov-dkms-git][AUR package]] so the Intel custom kernel is not required.  All we need is this AUR package, which is awesome.

After installing the AUR package, update kernel parameters to enable IOMMU and SR-IOV virtual functions.

#+begin_src conf
# /boot/loader/entries/arch.conf
options intel_iommu=on iommu=pt i915.enable_guc=7
#+end_src

Also add it to =mkinitcpio.conf=

#+begin_src conf
# /etc/mkinitcpio.conf
MODULES=(i915)
#+end_src

Then reboot the system.

** Create Virtual Functions
If hardware supports and configuration is correct, the following [[https://github.com/intel/linux-intel-lts/issues/33#issuecomment-1328970093][kernel message]] should be observed.

#+begin_src shell
$ sudo dmesg | grep i915_virtualization_probe
[    1.740640] i915 0000:00:02.0: i915_virtualization_probe: entry
#+end_src

Then identify the iGPU by =lspci=.  Usually Intel device starts with =00:02.x=.

#+begin_src shell
$ lspci | grep -P "VGA.*Intel"
00:02.0 VGA compatible controller: Intel Corporation Alder Lake-P Integrated Graphics Controller (rev 0c)
#+end_src

Then we should be able to get the number of virtual functions allowed currently.

#+begin_src shell
$ cat /sys/devices/pci0000:00/0000:00:02.0/sriov_numvfs
0
#+end_src

To enable virtual functions, run this command as root user.  I'm creating one virtual function since I only have one VM that needs it.  Maximal 7 virtual functions can be created.

#+begin_src shell
$ su
# echo 1 > /sys/devices/pci0000:00/0000:00:02.0/sriov_numvfs
#+end_src

Finally check VGA device again, there should be an additional device created (the one starts with =00:02.1=).

#+begin_src shell
$ lspci | grep -P "VGA.*Intel"
00:02.0 VGA compatible controller: Intel Corporation Alder Lake-P Integrated Graphics Controller (rev 0c)
00:02.1 VGA compatible controller: Intel Corporation Alder Lake-P Integrated Graphics Controller (rev 0c)
#+end_src

** Install Intel Graphics Driver
Add the virtual PCI graphics adapter to VM (choose the shared on =00:02.1=) and boot up Windows.  Be aware that we still the =QXL= video adapter this time.

Windows should now recognize a *Display adapters/Microsoft Basic Display Adapter* in /Device Manager/ if everything is correct.  Go to Intel website to get [[https://www.intel.ca/content/www/ca/en/download-center/home.html][the latest driver]].

Install it and reboot Windows.  That adapter should now be recognized as *Intel(R) Iris(R) Xe Graphics*.

* Install Looking Glass
Since I'm setting this up on a laptop I don't have external display so =Looking Glass= must be used to redirect the VM display on the laptop screen.

** Create Shared Memory
When the Windows VM is powered off, add the following content to the VM XML config (be aware of the hierarchy).

#+begin_src xml
<domain>
  <device>
    <shmem name='looking-glass'>
      <model type='ivshmem-plain'/>
      <size unit='M'>64</size>
    </shmem>
  </device>
</domain>
#+end_src

The size of shared memory depends on the [[https://looking-glass.io/docs/B5.0.1/install/#determining-memory][maximal resolution]] that Windows VM will be using.  Simply this formula can be used.  The result must be rounded up to the nearest power of 2.

#+begin_example
total bytes = width x height x 4 x 2 / 1024 / 1024 + 10
#+end_example

For example my Windows VM can setup maximal 2560x1600 so it would be /2560 x 1600 x 4 x 2 / 1024 / 1024 + 10 = 41.25/.  Then round it up to *64*.

After that, create a temp file config.  The user name must match the user that is being used.

#+begin_src conf
# /etc/tmpfiles.d/10-looking-glass.conf
#Type Path                   Mode UID  GID Age Argument
f     /dev/shm/looking-glass 0660 user kvm -
#+end_src

Create the temp file for the first time (it will be created automatically after reboot).

#+begin_src shell
$ sudo systemd-tmpfiles --create /etc/tmpfiles.d/10-looking-glass.conf
#+end_src

** Install Host Service
Download and install Looking Glass [[https://looking-glass.io/downloads][service installer]] in Windows.  Note that Windows is the host in Looking Glass's context (Linux is the client that reads output from Windows VM).

Additionally, =IVSHMEM= [[https://looking-glass.io/docs/B5.0.1/install/#installing-the-ivshmem-driver][driver]] is also needed so that Looking Glass service can shared the display via shared memory.  It can be downloaded from [[https://fedorapeople.org/groups/virt/virtio-win/direct-downloads/upstream-virtio/][RedHat]].

Then go to /Device Manager/ and find *System Devices/PCI standard RAM Controller*.  Update its driver with the downloaded =IVSHMEM= driver.

** Solve Display Adapter Error Code 43
Similarly I got the same error code as the passthrough for discrete graphic card.  To solve it, add the following content to the VM config XML.

#+begin_src xml
<domain>
  <features>
    <hyperv>
      <vendor_id state='on' value='1234567890ab'/>
    </hyperv>
    <kvm>
      <hidden state='on'>
    </kvm>
  </features>
</domain>
#+end_src

** Create a Dummy Display
Looking Glass mirrors display output so a display monitor must be connected to the eGPU's output.  If you don't have a dummy HDMI dongle, a fake display device can be installed.

Download the [[https://github.com/ge9/IddSampleDriver][IddSampleDriver]] from it's release page.
- Extract all content to =C:\IddSampleDriver= (Important since the path of config file is hard-coded).
- Run =C:\installCert.bat= with Administrator privilege.
- Install display driver: /Device Manager/ -> /Add legacy haradware/ -> /Advanced install/ -> /Display adapter/ -> /Browse driver on the disk/ -> /Choose the inf file in the extracted directory/.
- Restart Windows.

** Finish up
Now get the same version of [[https://looking-glass.io/downloads][Looking Glass client]] (important).  Compile and run.  The VM should be running smoothly with graphic acceleration.

* Pinned CPU Cores
To reduce latency, I prefer manually allocating CPU cores for both host and guest.  One thing needs to mention is this doesn't prevent host process from running on the pinned cores.  If the cores are expected to be used by VM only, consider [[https://wiki.archlinux.org/title/PCI_passthrough_via_OVMF#Isolating_pinned_CPUs][isolate pinned cores]].

First find out how many cores the CPU has.

#+begin_src shell
$ lscpu -e
CPU NODE SOCKET CORE L1d:L1i:L2:L3 ONLINE    MAXMHZ   MINMHZ       MHZ
  0    0      0    0 0:0:0:0          yes 4400.0000 400.0000  594.1430
  1    0      0    0 0:0:0:0          yes 4400.0000 400.0000  987.0030
  2    0      0    1 4:4:1:0          yes 4400.0000 400.0000 1144.1160
  3    0      0    1 4:4:1:0          yes 4400.0000 400.0000 2100.0000
  4    0      0    2 8:8:2:0          yes 4400.0000 400.0000  944.5790
  5    0      0    2 8:8:2:0          yes 4400.0000 400.0000  975.3940
  6    0      0    3 12:12:3:0        yes 4400.0000 400.0000 1116.9910
  7    0      0    3 12:12:3:0        yes 4400.0000 400.0000  829.3560
  8    0      0    4 16:16:4:0        yes 3300.0000 400.0000 1250.2290
  9    0      0    5 17:17:4:0        yes 3300.0000 400.0000 1150.0020
 10    0      0    6 18:18:4:0        yes 3300.0000 400.0000 1208.8120
 11    0      0    7 19:19:4:0        yes 3300.0000 400.0000 1016.9800
 12    0      0    8 20:20:5:0        yes 3300.0000 400.0000 1226.8660
 13    0      0    9 21:21:5:0        yes 3300.0000 400.0000 1046.9780
 14    0      0   10 22:22:5:0        yes 3300.0000 400.0000 1068.5780
 15    0      0   11 23:23:5:0        yes 3300.0000 400.0000 2100.0000
#+end_src

My CPU is Intel 1240P.  It has 4 performance cores and 8 efficiency cores.  In my setup, I keep core 0 for host and pass the rest 3 performance cores and 2 efficiency cores to the guest (for 8 threads).

Add this configuration to the VM.

#+begin_src xml
<vcpu placement="static">8</vcpu>
<iothreads>1</iothreads>
<cputune>
  <vcpupin vcpu="0" cpuset="2"/>
  <vcpupin vcpu="1" cpuset="3"/>
  <vcpupin vcpu="2" cpuset="4"/>
  <vcpupin vcpu="3" cpuset="5"/>
  <vcpupin vcpu="4" cpuset="6"/>
  <vcpupin vcpu="5" cpuset="7"/>
  <vcpupin vcpu="6" cpuset="8"/>
  <vcpupin vcpu="7" cpuset="9"/>
  <emulatorpin cpuset="0-1"/>
  <iothreadpin iothread="1" cpuset="0-1"/>
</cputune>
<cpu mode="host-passthrough" check="none" migratable="on">
  <topology sockets="1" dies="1" cores="4" threads="2"/>
</cpu>
#+end_src

* Enable MSI(Message Signaled-Based Interrupts)
Most likely by default the Windows VM will use /Line-Base Interrupts/ which usually causes audio distortion and slow down the performance.  In this case, we need to enable MSI.

To check what type of interrupts the graphic card is using at the moment, run this command as root while VM is running.  =04:00= is my graphic card bus ID.

#+begin_src shell
$ sudo lspci -vs 04:00 | grep 'MSI:'
Capabilities: [68] MSI: Enable+ Count=1/1 Maskable- 64bit+
Capabilities: [68] MSI: Enable+ Count=1/1 Maskable- 64bit+
#+end_src

A =-= after Enable [[https://wiki.archlinux.org/title/PCI_passthrough_via_OVMF#Slowed_down_audio_pumped_through_HDMI_on_the_video_card][means]] MSI is supported, but not used by the virtual machine, while a =+= says that the virtual machine is using it.  Since I have enabled it already the result shows =+= which is expected.

Then go back to the Windows VM and [[https://forums.guru3d.com/threads/windows-line-based-vs-message-signaled-based-interrupts-msi-tool.378044/][enable]] MSI.

In /Device Manager/, find the graphic card: /View/ -> /Resource by type/ -> /Interrupt request (IRQ)/ -> /(PCI) 0xFFFFFFD0 (-48)  NVIDIA GeForce GTX 970/.

The value in the parenthesis also reflects the interrupt status.  Positive value means Line-Based Interrupt mode and negative value means Message Signaled-Based mode.  As my card has already been tweaked, it is negative value here.

To enable MSI: open the property of /(PCI) 0xFFFFFFD0 (-48)  NVIDIA GeForce GTX 970/ -> /Details/ -> in the /Property/ dropdown menu select /Device instance path/.

Note down the value of this property.  In my case it is =PCI\VEN_10DE&DEV_13C2&SUBSYS_367A1458&REV_A1\4&123CFF10&0&000C=.

Then open /Registry Editor/ and find out the device entry.  It should be a subkey of =Computer\HKEY_LOCAL_MACHINE\SYSTEM\CurrentControlSet\Enum=.  E.g. =PCI\VEN_10DE&DEV_13C2&SUBSYS_367A1458&REV_A1\4&123cff10&0&000C\Device Parameters\Interrupt Management=.  From there, create a new key named =MessageSignaledInterruptProperties= as well as a DWORD value =MSISupported= with data =1= (Change the data if it exists already).

In addition to the graphic card itself, the audio device on the card also needs to be tweaked.  Audio device should have the same vendor ID and a different device ID which can be obtained from [[*Identify Discrete Graphic Card][the previous section]].  Normally it should be adjacent to the card.  For example =Computer\HKEY_LOCAL_MACHINE\SYSTEM\CurrentControlSet\Enum\PCI\VEN_10DE&DEV_0FBB&SUBSYS_367A1458&REV_A1\4&336a283&0&0010\Device Parameters\Interrupt Management\MessageSignaledInterruptProperties=.  Similarly update the data to =1=.

Now reboot Windows and check if graphic card works from /Device Manager/ as well as the IRQ value which shows MSI status.

* Huge Memory Pages
Normally, I don't think I really need this setup.  In any case if it's needed, I'll use [[https://wiki.archlinux.org/title/PCI_passthrough_via_OVMF#Static_huge_pages][static huge pages]].  With static huge pages, a portion of memory will be allocated for VM and can't be used by host.

To verify is CPU supports it.

#+begin_src shell
$ grep pdpe1gb /proc/cpuinfo
#+end_src

Add kernel parameters.

#+begin_src conf
# /boot/loader/entries/arch.conf
options default_hugepagesz=1G hugepagesz=1G hugepages=X
#+end_src

Add this configuration to the VM.

#+begin_src xml
<memoryBacking>
  <hugepages/>
</memoryBacking>
#+end_src

* Audio Redirection
By default the audio is output from HDMI through the graphic card (if it supports).  In order to use the laptop speaker, we need to install a special virtual audio device called [[https://github.com/duncanthrax/scream][scream]].

On Arch, installing from [[https://aur.archlinux.org/packages/scream][AUR]] is the most convenient way.  Then start the service by receiving audio through network.  In this case, =virbr0= is my NAT adapter.

#+begin_src shell
$ scream -i virbr0
#+end_src

On Windows, download and install the [[https://github.com/duncanthrax/scream/releases][driver]] then switch output device to scream.  It should work perfectly out of the box.

** Alternative Way to Stream Audio
=scream= also supports stream through shared memory but it requires additional setup.  Checkout it's readme as it shows how to set it up.  Usually, I don't feel like I really need it.  If there is obvious latency, this should a backup plan.

* References
[[https://wiki.archlinux.org/title/QEMU][ArchWiki QEMU]]
[[https://wiki.archlinux.org/title/PCI_passthrough_via_OVMF][ArchWiki OVMF]]
[[https://wiki.archlinux.org/title/Intel_GVT-g][Intel GVT-g]]
[[https://looking-glass.io/][Looking Glass]]
[[https://github.com/virtio-win/virtio-win-pkg-scripts/blob/master/README.md][Virtio driver]]
[[https://github.com/intel/linux-intel-lts/issues/33][SR-IOV: mainlining?]]
[[https://github.com/strongtz/i915-sriov-dkms][strongtz/i915-sriov-dkms]]
[[https://github.com/ge9/IddSampleDriver][ge9/IddSampleDriver]]
[[https://www.reddit.com/r/VFIO/comments/wj6zhz/gpu_passthrough_looking_glass_no_external/][GPU Passthrough + Looking Glass + no external monitor/dummy]]
[[https://forums.guru3d.com/threads/windows-line-based-vs-message-signaled-based-interrupts-msi-tool.378044/][Windows: Line-Based vs. Message Signaled-Based Interrupts. MSI tool.]]
